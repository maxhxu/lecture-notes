\section{Day 1: Review and ADTs (Sept 3, 2025)}

\subsection{ADTs}

Abstract data types (ADTs) are a mathematical object, which has a set of operations. Examples include a set, a sequence, or a graph.

A \textit{data structure} that implements an ADT provides a representation for the object in memory and algorithms for each operation. There may exist many different data structures that implement the same ADT, with varying running time for each operation.

\begin{example}[Dictionary ADT]
    The ADT consists of:
    \begin{itemize}
    \item \textbf{Object}: a set of elements, with each having a unique key from totally ordered universe $U$
    \item \textbf{Operations}: 
    \begin{description}
    \item[Insert$(S, x)$] adds an element with key $x$ to the set $S$ if $S$ does not contain an element with key $x$
    \item[Delete$(S, x)$] removes an element with key $x$ from $S$ if it exists
    \item[Search$(S, x)$] returns a pointer to the element in $S$ with key $x$, or nil if such an element doesn't exist
    \end{description}
    \end{itemize}
\end{example}

Here we are not worried about key-value pairs, although a specific implementation of this ADT might. We could implement this as a set of keys, with no associated element for each key.

An example implementation of dictionary ADT using singly linked list (unsorted) can have the following properties:

    
\subsection{Review}

Let $t(x)$ be the number of steps taken by an algorithm $A$ on input $x$. \\
Let the worst case step complexity of $A$ be $T(n)$, where
\[
T(n) = \text{max}\{ t(x) \mid x \text{ is an input of size } n \}
\]
Typically, this is very hard to determine exactly, which is why asymptotic notation is instead, which still captures how quickly $T(n)$ grows with respect to $n$. 

$T(n) \in O(f(n))$ if there exists a constant $c, n_0 \in \mathbb{N}$, for all $n \in \mathbb{N}$ such that when $n > n_0$, $T(n) \leq c f(n)$.
$T(n) \in \Omega(f(n))$ if there exists a constant $c, n_0 \in \mathbb{N}$, for all $n \in \mathbb{N}$ such that when $n > n_0$, $c f(n) \leq T(n)$.
$T(n) \in \Theta(f(n))$ if $T(n) \in O(f(n))$ and $T(n) \in \Omega(f(n))$.

For upper bound, show that there exists positive constant $c$ and for all $n$ large enough, for every input of size $n$, the algorithm takes at most $cf(n)$ steps. We write $T(n) \in O(n)$.

For lower bound, show that there exists positive constant $c$ and for all $n$ large enough, there exists some input of size $n$ that makes the algorithm take at least $cf(n)$ steps. We write $T(n) \in \Omega(n)$.

\newpage
\subsection*{BubbleSort($A[1...n]$)}
\begin{algorithmic}[1]
\State last = $n$ 
\State sorted = False
\While{not sorted}
    \State sorted = True
    \For{$j=1$ to $\text{last} - 1$}
        \If{$A[j] > A[j+1]$}
            \State swap $A[j]$ and $A[j+1]$
            \State sorted = False
        \EndIf
    \EndFor
    \State last = $\text{last} - 1$
\EndWhile
\end{algorithmic}

\subsubsection*{Upper Bound}

The outer loop can occur at most $n$ times, as last starts at $n$ and decrements by 1 every time the outer loop runs. The inner loop goes from $1$ to $\text{last} - 1$, meaning there are at most $\text{last} - 1$ iterations of the inner loop. This can be written as the sum
\[
\sum_{i=1}^{n} (i-1) = \left(\sum_{i=1}^{n} i\right) - n = \frac{n(n+1)}{2} - n = \frac{n^2 - n}{2}
\]
meaning for this algorithm $T(n) \in O(n^2)$.

\subsubsection*{Lower Bound}

We can pick the list $A = [n, n-1, \dots, 2, 1]$. This list has the property that the first element is the largest number, barring the sorted block of size $i - 1$ present at the end of the array at the start of the $i$-th iteration.\footnote{i think iterations start at 1 in this course?} This means that it will take $n - i$ swaps, which means it takes
\[
\sum_{i=1}^{n} (n - i) = n^2 - \frac{n(n+1)}{2} = \frac{n^2 - n}{2}
\]
steps total, meaning for this algorithm $T(n) \in \Omega(n^2)$.

\subsubsection*{Theta Bound}
As $T(n) \in \Omega(n^2)$ and $T(n) \in O(n^2)$, we have $T(n) \in \Theta(n^2)$. 
